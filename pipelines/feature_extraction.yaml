name: Feature extraction
inputs:
- {name: project, type: String}
- {name: location, type: String}
- {name: images_gcs_uri, type: String}
- {name: emb_index_gcs_uri, type: String}
- {name: saved_model_gcs_bucket, type: String}
- {name: index_json_name, type: String}
outputs:
- {name: embedding_index_file_uri, type: String}
- {name: saved_pretrained_model_gcs_location, type: String}
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-aiplatform==1.16.1' 'google-cloud-storage' 'tensorflow==2.8' 'tensorflow-hub==0.12.0' 'tensorflow-estimator==2.8.0' 'keras==2.8.0' 'kfp==1.8.13' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef feature_extraction(\n    project: str,\n    location: str,\n\
      \    images_gcs_uri: str,\n    emb_index_gcs_uri: str,\n    saved_model_gcs_bucket:\
      \ str,\n    index_json_name: str,\n    # model_resource_path: str,\n    # vertex_model_gcs_dir:\
      \ str,\n) -> NamedTuple('Outputs', [('embedding_index_file_uri', str),\n   \
      \                         ('saved_pretrained_model_gcs_location', str),\n  \
      \                          # ('img_bottleneck_model', Artifact),\n         \
      \                   ]):\n  import os\n  from google.cloud import storage\n \
      \ from google.cloud.storage.bucket import Bucket\n  from google.cloud.storage.blob\
      \ import Blob\n  from datetime import datetime\n  import tensorflow as tf\n\
      \  import tensorflow_hub as hub\n\n\n  # Load compressed models from tensorflow_hub\n\
      \  os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n\n  # TODO: paramaterize\n\
      \  IMG_HEIGHT = 224\n  IMG_WIDTH = 224\n  IMG_CHANNELS = 3\n  BATCH_SIZE = 32\n\
      \  NUM_IMAGES = 510\n  NUM_NEIGH = 3 # top 3\n\n  # ==============================================================================\n\
      \  # Define helper functions\n  # ==============================================================================\n\
      \n  def _upload_blob_gcs(gcs_uri, source_file_name, destination_blob_name):\n\
      \    \"\"\"Uploads a file to GCS bucket\"\"\"\n    client = storage.Client(project=project)\n\
      \    blob = Blob.from_string(os.path.join(gcs_uri, destination_blob_name))\n\
      \    blob.bucket._client = client\n    blob.upload_from_filename(source_file_name)\n\
      \n  def read_and_decode(filename, reshape_dims=[IMG_HEIGHT, IMG_WIDTH]):\n \
      \   # Read the file\n    img = tf.io.read_file(filename)\n\n    # Convert the\
      \ compressed string to a 3D uint8 tensor.\n    img = tf.image.decode_jpeg(img,\
      \ channels=IMG_CHANNELS)\n\n    # Use `convert_image_dtype` to convert to floats\
      \ in the [0,1] range.\n    # This makes the img 1 x 224 x 224 x 3 tensor with\
      \ the data type of float32\n    img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis,\
      \ ...]\n\n    # Resize the image to the desired size.\n    return tf.image.resize(img,\
      \ reshape_dims)\n\n  def create_embeddings_dataset(embedder, img_path):\n  \
      \  dataset_filenames = []\n    dataset_embeddings = []\n\n    list_dir = tf.io.gfile.listdir(img_path)\n\
      \n    for file in list_dir:\n      img_tensor = read_and_decode(img_path + \"\
      /\" + file, [IMG_WIDTH, IMG_HEIGHT])\n      embeddings = embedder(img_tensor)\n\
      \      dataset_filenames.append(img_path + \"/\" + file)\n      dataset_embeddings.extend(embeddings)\n\
      \n    dataset_embeddings = tf.convert_to_tensor(dataset_embeddings)\n\n    return\
      \ dataset_filenames, dataset_embeddings\n\n  # ==============================================================================\n\
      \  # Download pre-trained model\n  # ==============================================================================\n\
      \  layers = [\n      hub.KerasLayer(\n          \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\
      , # TODO: paramaterize\n          input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n\
      \          trainable=False,\n          name='mobilenet_embedding'),\n      tf.keras.layers.Flatten()\n\
      \  ]\n\n  model = tf.keras.Sequential(layers, name='pretrained_mobilenet') #\
      \ TODO: paramaterize\n  # loaded_model = tf.keras.models.load_model(vertex_model_gcs_dir)\n\
      \  # print(\"model summary:\", loaded_model.summary())\n\n  TIMESTAMP = datetime.now().strftime(\"\
      %Y%m%d%H%M%S\")\n  MODEL_NAME = f'pipe-mobilenet_v2-{TIMESTAMP}'\n  print(\"\
      MODEL_NAME\", MODEL_NAME)\n\n  save_path = os.path.join(saved_model_gcs_bucket,\
      \ MODEL_NAME) # \"gs://\", \n  print(\"model save_path\", save_path)\n\n  model.save(save_path)\n\
      \n  # ==============================================================================\n\
      \  # Create embedding dataset\n  # ==============================================================================\n\
      \  dataset_filenames, dataset_embeddings = create_embeddings_dataset(\n    lambda\
      \ x: model.predict(x),\n    images_gcs_uri\n  )\n\n  print(\"sample dataset_filenames\"\
      , dataset_filenames[:3])\n  print(\"dataset_embeddings shape:\", dataset_embeddings.shape)\n\
      \n  # ==============================================================================\n\
      \  # Write Embeddings and IDs to json\n  # ==============================================================================\n\
      \n  # TODO: this code will only work with the file pattern created from zipped\
      \ file\n  #       adjust this to expected GCS file patterns\n\n  with open(f\"\
      {index_json_name}\", \"w\") as f:\n    for gcs_uri, vector in zip(dataset_filenames,dataset_embeddings):\n\
      \      x = gcs_uri.split(\"/\")[-1]\n      id = x.split(\".\")[0]\n      vector\
      \ = vector.numpy()\n      f.write('{\"id\":\"' + str(id) + '\",')\n      f.write('\"\
      embedding\":[' + \",\".join(str(x) for x in vector) + \"]}\")\n      f.write(\"\
      \\n\")\n\n  _upload_blob_gcs(emb_index_gcs_uri, f\"{index_json_name}\", f\"\
      {index_json_name}\") \n\n  embedding_index_file_uri = f'{emb_index_gcs_uri}/{index_json_name}'\n\
      \  print(\"embedding_index_file_uri:\", embedding_index_file_uri)\n\n  return(\n\
      \      f'{embedding_index_file_uri}',\n      f'{save_path}',\n  )\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - feature_extraction
