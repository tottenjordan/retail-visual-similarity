{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a3a8f1a-efa4-4598-83e4-c1566d73a9ea",
   "metadata": {},
   "source": [
    "# Local Test Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfe7a0-9b8f-4e74-ae07-8038bb6aa9ec",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744dc85-e711-4c1f-b9b7-cdeafe6a6a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load compressed models from tensorflow_hub\n",
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "# prepare images for expected input\n",
    "def read_and_decode(filename, reshape_dims=[IMG_HEIGHT, IMG_WIDTH]):\n",
    "  img = tf.io.read_file(filename)\n",
    "  img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
    "  return tf.image.resize(img, reshape_dims)\n",
    "\n",
    "# Download model from TF Hub\n",
    "layers = [\n",
    "      hub.KerasLayer(\n",
    "          \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\",\n",
    "          input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "          trainable=False,\n",
    "          name='mobilenet_embedding'),\n",
    "      tf.keras.layers.Flatten()\n",
    "]\n",
    "model = tf.keras.Sequential(layers, name='z_embedding')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b1053-75dd-4420-bebe-de1be69010f2",
   "metadata": {},
   "source": [
    "### Vector attributes / labels\n",
    "\n",
    "[specify namespaces and tokens](https://cloud.google.com/vertex-ai/docs/matching-engine/filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b124c-9cb5-4026-a45c-869b56ff5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = {\"id\": \"42\", \n",
    "#         \"embedding\": [0.5, 1.0], \n",
    "#         \"restricts\": [\n",
    "#                       {\n",
    "#                           \"namespace\": \"class\",\"allow\": [\"cat\", \"pet\"]\n",
    "#                       },\n",
    "#                       {\n",
    "#                           \"namespace\": \"category\", \"allow\": [\"feline\"]\n",
    "#                        }\n",
    "#                       ]\n",
    "#         }\n",
    "\n",
    "# v_attr = {\n",
    "#     \"id\": \"43\", \n",
    "#     \"embedding\": [0.6, 1.0], \n",
    "#     \"restricts\": [\n",
    "#                   {\"namespace\":\"class\", \"allow\": [\"dog\", \"pet\"]},\n",
    "#                   {\"namespace\": \"category\", \"allow\":[\"canine\"]}\n",
    "#     ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60fbf9-9ff2-4e53-9279-c04f966307a6",
   "metadata": {},
   "source": [
    "### Create Query embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bece91d-760d-40cb-94c6-d13afc625c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TEST_SAMPLES = 50\n",
    "# EVAL_IMG_PATH = 'gs://retail-product-img-kaggle/dataset/test/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1707f024-f0c2-413d-aa3d-42c7cb6f4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_embeddings(embedder, img_path, num_test_samples):\n",
    "  dataset_filenames = []\n",
    "  dataset_embeddings = []\n",
    "  \n",
    "  list_dir = tf.io.gfile.listdir(img_path)\n",
    "  \n",
    "  for file in list_dir[:num_test_samples]:\n",
    "    img_tensor = read_and_decode(img_path + \"/\" + file, [IMG_WIDTH, IMG_HEIGHT])\n",
    "    embeddings = embedder(img_tensor)\n",
    "    dataset_filenames.append(img_path + \"/\" + file)\n",
    "    dataset_embeddings.extend(embeddings)\n",
    "  \n",
    "  dataset_embeddings = tf.convert_to_tensor(dataset_embeddings)\n",
    "  \n",
    "  return dataset_filenames, dataset_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c73452-2fb0-4663-8270-c4713e8570b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_filenames, query_embeddings = create_query_embeddings(\n",
    "    lambda x: model.predict(x),\n",
    "    EVAL_IMG_PATH,\n",
    "    NUM_TEST_SAMPLES\n",
    ")\n",
    "\n",
    "vector_list = []\n",
    "for q_vector in query_embeddings:\n",
    "  vector_list.append(q_vector.numpy())\n",
    "\n",
    "print(\"query_filenames:\", query_filenames)\n",
    "print(\"query_embeddings shape:\", query_embeddings.shape) # should be (NUM_TEST_SAMPLES, 1280)\n",
    "# print(\"vector_list shape:\", vector_list.shape)\n",
    "vector_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9faaed-9072-4a32-a06c-293565ad6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17882c-c4b5-4c9c-8f20-f6efc062536e",
   "metadata": {},
   "source": [
    "### Query ME Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a63685-2027-45b6-8caf-75b268ed51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai index-endpoints list --project=\"jtotten-project\" --region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b64766-4864-4634-8234-2d938feeff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_endpoint_resource_uri = 'projects/163017677720/locations/us-central1/indexEndpoints/5129564791202906112'\n",
    "index_endpoint = aiplatform.MatchingEngineIndexEndpoint(index_endpoint_resource_uri)\n",
    "\n",
    "NUM_NEIGH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0275f9f-e2fa-4dd4-a304-57a49f20a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_ann_index_name = 'ann_1280_deployed_index_kg_retail-20220223223806'\n",
    "brute_index_resource_path = 'projects/163017677720/locations/us-central1/indexes/6397380862865833984'\n",
    "\n",
    "ann_response = index_endpoint.match(\n",
    "    deployed_index_id=deployed_ann_index_name, \n",
    "    queries=vector_list, \n",
    "    num_neighbors=NUM_NEIGH\n",
    ")\n",
    "\n",
    "print(\"ann_response:\", ann_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823f97b-956e-43e5-a912-914e86c248a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_brute_index_name = 'brute_force_1280_deployed_index_kg_retail-20220223222939'\n",
    "brute_index_resource_path = 'projects/163017677720/locations/us-central1/indexes/1062867104245481472'\n",
    "\n",
    "brute_force_response = index_endpoint.match(\n",
    "    deployed_index_id=deployed_brute_index_name, \n",
    "    queries=vector_list, \n",
    "    num_neighbors=NUM_NEIGH\n",
    ")\n",
    "\n",
    "print(\"brute_force_response:\", brute_force_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e028f-52f3-4641-8b19-5ad26d327a8e",
   "metadata": {},
   "source": [
    "### Visualize Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a88032-06d4-4128-b7c9-a383d2e6abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_IMAGES = 510\n",
    "NUM_NEIGH = 3 # 3, 10, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed799dad-bed9-4cbe-b1ec-26f9b1739a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_to_plot(filename, reshape_dims=[IMG_HEIGHT, IMG_WIDTH]):\n",
    "  img = tf.io.read_file(filename)\n",
    "  img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32) # removed axis from previous\n",
    "  return tf.image.resize(img, reshape_dims)\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(len(query_filenames), NUM_NEIGH + 1,\n",
    "                     figsize=(5 * (1 + NUM_NEIGH), 5 * len(query_filenames)))\n",
    "\n",
    "for rowno, query_filename in enumerate(query_filenames):\n",
    "  ax[rowno][0].imshow(decode_to_plot(query_filename).numpy())\n",
    "  ax[rowno][0].axis('off')\n",
    "  for colno, neigh in enumerate(neighbors[rowno]):                                      # TODO: change neighbors to responses?\n",
    "    ax[rowno][colno+1].imshow(decode_to_plot(query_filenames[neigh]).numpy())           # TODO: query_filenames |  dataset_filenames\n",
    "    ax[rowno][colno+1].set_title('dist={:.1f}'.format(distances[rowno][colno].numpy())) # TODO: fix\n",
    "    ax[rowno][colno+1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9cf0c-bdf7-4857-a9fe-6bf4ff814492",
   "metadata": {},
   "source": [
    "### Compute Recall\n",
    "\n",
    "Use deployed brute force Index as the ground truth to calculate the recall of ANN Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889f064-7732-4834-9bb4-0c55277699f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NEIGH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29608f4a-8d84-46ed-b32e-5fd437816745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve nearest neighbors for both the tree-AH index and the brute-force index\n",
    "\n",
    "deployed_ann_index_name = 'ann_1280_deployed_index_kg_retail-20220223223806'\n",
    "brute_index_resource_path = 'projects/163017677720/locations/us-central1/indexes/6397380862865833984'\n",
    "\n",
    "\n",
    "# Retrieve nearest neighbors for both the tree-AH index and the brute-force index\n",
    "ann_response_test = index_endpoint.match(\n",
    "    deployed_index_id=deployed_ann_index_name, \n",
    "    queries=vector_list, \n",
    "    num_neighbors=NUM_NEIGH\n",
    ")\n",
    "\n",
    "# Brute Force Index\n",
    "deployed_brute_index_name = 'brute_force_1280_deployed_index_kg_retail-20220223222939'\n",
    "brute_index_resource_path = 'projects/163017677720/locations/us-central1/indexes/1062867104245481472'\n",
    "\n",
    "brute_force_response_test = index_endpoint.match(\n",
    "    deployed_index_id=deployed_brute_index_name, \n",
    "    queries=vector_list, \n",
    "    num_neighbors=NUM_NEIGH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549676c4-0670-4b6f-95cb-f01a0f635d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall by determining how many neighbors correctly retrieved, compared to brute-force method.\n",
    "\n",
    "correct_neighbors = 0\n",
    "\n",
    "for tree_ah_neighbors, brute_force_neighbors in zip(ann_response_test, brute_force_response_test):\n",
    "    tree_ah_neighbor_ids = [neighbor.id for neighbor in tree_ah_neighbors]\n",
    "    brute_force_neighbor_ids = [neighbor.id for neighbor in brute_force_neighbors]\n",
    "    \n",
    "    correct_neighbors += len(set(tree_ah_neighbor_ids).intersection(brute_force_neighbor_ids))\n",
    "\n",
    "recall = correct_neighbors / (len(vector_list) * NUM_NEIGH)\n",
    "\n",
    "print(\"Recall: {}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b81817-3770-400b-9fe6-14002f6346ea",
   "metadata": {},
   "source": [
    "### Create local model for testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b3a14-b3cd-411f-a1e2-b74eea7f898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUCKET = \"retail-product-kaggle\"\n",
    "# save_path = os.path.join(\"gs://\", gcp_bucket, f'saved_models/mobilenet_v2')\n",
    "aiplatform.init(project=PROJECT_ID,location=LOCATION,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4d406-6aa1-4841-a57d-ae4823807348",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "BATCH_SIZE = 32\n",
    "NUM_IMAGES = 510\n",
    "NUM_NEIGH = 3 # top 3\n",
    "\n",
    "layers = [\n",
    "      hub.KerasLayer(\n",
    "          \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\",\n",
    "          input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "          trainable=False,\n",
    "          name='mobilenet_embedding'),\n",
    "      tf.keras.layers.Flatten()\n",
    "]\n",
    "local_model = tf.keras.Sequential(layers, name='z_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f94d33-4c8c-4832-879c-be05f6d63578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_decode(filename, reshape_dims=[IMG_HEIGHT, IMG_WIDTH]):\n",
    "  img = tf.io.read_file(filename)\n",
    "  img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
    "  return tf.image.resize(img, reshape_dims)\n",
    "\n",
    "def create_embeddings_dataset(embedder, img_path):\n",
    "  dataset_filenames = []\n",
    "  dataset_embeddings = []\n",
    "  list_dir = tf.io.gfile.listdir(img_path)\n",
    "  for file in list_dir:\n",
    "    img_tensor = read_and_decode(img_path + \"/\" + file, [IMG_WIDTH, IMG_HEIGHT])\n",
    "    embeddings = embedder(img_tensor)\n",
    "    dataset_filenames.append(img_path + \"/\" + file)\n",
    "    dataset_embeddings.extend(embeddings)\n",
    "  \n",
    "  dataset_embeddings = tf.convert_to_tensor(dataset_embeddings)\n",
    "  \n",
    "  return dataset_filenames, dataset_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d046f-5804-40f1-8c5d-0d9f2798f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = f'gs://{BUCKET}/extract/image_data_500_images_500_data_100230683.0.jpg'\n",
    "read_and_decode(IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67327cdf-87a0-42fd-8d41-235bfcbba257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded = tf.saved_model.load(MODEL_DIR)\n",
    "# loaded_k = tf.keras.models.load_model(MODEL_DIR)\n",
    "\n",
    "IMG_PATH = f'gs://{BUCKET}/extract'\n",
    "\n",
    "dataset_filenames, dataset_embeddings = create_embeddings_dataset(\n",
    "    lambda x: local_model.predict(x),\n",
    "    IMG_PATH\n",
    ")\n",
    "\n",
    "print(dataset_filenames[:3])\n",
    "print(dataset_embeddings.shape) # should be (NUM_IMAGES, 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a04b3-8517-4afc-bd87-48402eb367d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = read_and_decode(IMG_PATH)\n",
    "# json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a00c0-f8c1-41cd-ac86-8e320346820a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0315567-94b6-4799-b911-255637a513b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b5c16-3691-49cd-9f38-1dff9ce9beb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa13e6-c623-443f-aa17-04f0b420af95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
