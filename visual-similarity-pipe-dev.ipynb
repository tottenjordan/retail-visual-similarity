{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f80527-62db-43e5-9b3c-b7746357672f",
   "metadata": {},
   "source": [
    "**Notebook still in development**\n",
    "* Search `TODO` \n",
    "* Current example uses a small image dataset; plan to adapt for ~ImageNet\n",
    "* Need custom serving container or prediction routine to convert query images to model input in json format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e085e42-2573-4bce-ad2b-95152a13d060",
   "metadata": {},
   "source": [
    "# Image Similarity Matching / Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0d08f-b28a-404a-9847-fd80ab0d05b6",
   "metadata": {},
   "source": [
    "1. Use a pretrained deep learning model to extract feature (embedding) vectors for each image in a product catalog\n",
    "\n",
    "2. Store embedding vectors in a scalable appriximate nearest neighbor (ANN) service, [Vertex Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview), where each image's embedding vectors are indexed by product ID\n",
    "\n",
    "3. To retrieve the most similar images to a query image, use same pretrained model from (1) to extract the embedding vectors from the query image and query the index with these vectors\n",
    "\n",
    "#### This notebook orchestrates the below pipeline steps with [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b057566d-8350-42c4-8bfa-b106af2fd06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![End-to-End Vertex Pipeline](https://github.com/tottenjordan/retail-visual-similarity/blob/master/img/pipeline-v1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7617c29-661f-4245-a56a-09e627358f1f",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Matching Engine [SDK source code](https://github.com/googleapis/python-aiplatform/tree/main/google/cloud/aiplatform/_matching_engine)\n",
    "* TODO: remove when complete: Original [Colab](https://colab.sandbox.google.com/drive/1ysjjGTv7EKkBD90dsdVD3OZvW4Oka1aC#scrollTo=AIaRc1NI3M6P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e5ec8-3325-4e47-9731-3ad9ea2080bd",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a032636-cea4-462f-87f6-6fab0cf4fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'  # <--- TODO: CHANGE THIS\n",
    "LOCATION = 'us-central1' \n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b2fd35-16b4-4996-87bb-052b948a64bd",
   "metadata": {},
   "source": [
    "If using Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2288ffe3-405d-4e28-a095-51be83107372",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# if 'google.colab' in sys.modules:\n",
    "#     from google.colab import auth\n",
    "#     auth.authenticate_user()\n",
    "    \n",
    "    \n",
    "# if 'google.colab' in sys.modules:\n",
    "#     USER_FLAG = ''\n",
    "# else:\n",
    "#     USER_FLAG = '--user'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625962fb-1d11-463b-9c4c-8678e7fdcfc0",
   "metadata": {},
   "source": [
    "## pip & package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89edf23d-0232-400e-9cde-20f40c2f4802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use these\n",
    "# ! pip3 install -U google-cloud-storage $USER_FLAG\n",
    "! pip3 install $USER kfp google-cloud-pipeline-components --upgrade\n",
    "# !pip install google-cloud-aiplatform==1.16.1\n",
    "\n",
    "# (Optional) OSS Scann for testing\n",
    "# !pip install scann\n",
    "\n",
    "# TODO: Not Needed (?)\n",
    "# !git clone https://github.com/kubeflow/pipelines.git\n",
    "# !pip install pipelines/components/google-cloud/.\n",
    "\n",
    "\n",
    "# Automatically restart kernel after installs\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d6ad11-ed47-483b-b19e-ecad0930f864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.13\n",
      "google_cloud_pipeline_components version: 1.0.17\n",
      "aiplatform SDK version: 1.16.1\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f8abb4e-cea9-43e6-a4d0-cbf357e943a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import sys\n",
    "\n",
    "# Display Images\n",
    "from IPython.display import clear_output, Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# GCP\n",
    "from google.cloud import aiplatform\n",
    "# from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Pipelines\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "# Kubeflow SDK\n",
    "# TODO: fix these\n",
    "from kfp.v2 import dsl\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec86f89-f13e-4133-8094-c74d90674f03",
   "metadata": {},
   "source": [
    "### Setup Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e99aabeb-7247-4b0a-8cd4-06d3bdb09cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS_PIPELINE_ROOT_PATH: gs://retail-products-kaggle/pipeline_root/jtott\n",
      "BUCKET_URI: gs://retail-products-kaggle\n"
     ]
    }
   ],
   "source": [
    "# Required Pipeline Parameters\n",
    "PIPE_USER = 'jtott'  #  {type: 'string'} <--- TODO: CHANGE THIS\n",
    "BUCKET = 'retail-products-kaggle'\n",
    "BUCKET_URI = f'gs://{BUCKET}'\n",
    "DATA_FOLDER = 'data-full'\n",
    "\n",
    "GS_PIPELINE_ROOT_PATH = 'gs://{}/pipeline_root/{}'.format(BUCKET, PIPE_USER)\n",
    "\n",
    "LOCATION = 'us-central1'\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "# !gcloud config set project {PROJECT_ID}\n",
    "\n",
    "print('GS_PIPELINE_ROOT_PATH: {}'.format(GS_PIPELINE_ROOT_PATH))\n",
    "print('BUCKET_URI: {}'.format(BUCKET_URI))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71793878-ed0c-4c72-a1b0-c0591059c1f7",
   "metadata": {},
   "source": [
    "#### create GCS bucket if not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89e0facd-cea1-43d3-8997-401f6a45fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2c1480-d06a-4acf-863a-02b60585f210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 271604965  2022-08-17T01:48:43Z  gs://retail-products-kaggle/retail-products-classification.zip#1660700923141049  metageneration=1\n",
      "                                 gs://retail-products-kaggle/data-full/\n",
      "                                 gs://retail-products-kaggle/dataset/\n",
      "TOTAL: 1 objects, 271604965 bytes (259.02 MiB)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b7d4446-5b9a-4756-92c8-44b248ba4d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/google/client/client.py:173: FutureWarning: AIPlatformClient will be deprecated in v2.0.0. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Setup Clients\n",
    "os.environ['GOOGLE_CLOUD_PROJECT']=PROJECT_ID\n",
    "\n",
    "# colab_auth.authenticate_user() # if using colab\n",
    "\n",
    "storage_client = storage.Client(\n",
    "    project=PROJECT_ID\n",
    ")\n",
    "# PipelineJob\n",
    "pipeline_client = pipelines_client.AIPlatformClient(\n",
    "  project_id=PROJECT_ID,\n",
    "  region=LOCATION,\n",
    ")\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c775d-900d-443b-9995-d0bbd3bc2003",
   "metadata": {},
   "source": [
    "### Saving Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34912fa0-adc1-49a5-9d7d-3093bac3ae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINES_FILEPATH: gs://retail-products-kaggle/pipelines/pipelines.json\n"
     ]
    }
   ],
   "source": [
    "# Pipeline Stuff\n",
    "PIPELINES = {}\n",
    "\n",
    "PIPELINES_FILEPATH = f'gs://{BUCKET}/pipelines/pipelines.json' # <--- TODO: CHANGE THIS; can be blank json file\n",
    "print(\"PIPELINES_FILEPATH:\", PIPELINES_FILEPATH)\n",
    "\n",
    "if os.path.isfile(PIPELINES_FILEPATH):\n",
    "  with open(PIPELINES_FILEPATH) as f:\n",
    "    PIPELINES = json.load(f)\n",
    "else:\n",
    "  PIPELINES = {}\n",
    "\n",
    "def save_pipelines():\n",
    "  with open(PIPELINES_FILEPATH, 'w') as f:\n",
    "    json.dump(PIPELINES, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623d199-3342-4dda-9fc6-cf52f02f1a5f",
   "metadata": {},
   "source": [
    "# Create Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a2841-c24a-4814-ba23-c5c9d2e3f4d4",
   "metadata": {},
   "source": [
    "## Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04e144b1-23d2-4e93-a44c-a969960b2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf pipelines\n",
    "!mkdir -p ./pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c723c-747a-4289-bfb6-03355e479836",
   "metadata": {},
   "source": [
    "### Find Model Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec8cea06-5291-4a5c-b35f-4300b34d5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=['google-cloud-aiplatform==1.16.1'],\n",
    "  output_component_file=\"./pipelines/find_model_endpoint.yaml\",\n",
    ")\n",
    "def find_model_endpoint_test(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    endpoint_name: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "                            ('create_new_endpoint', str),\n",
    "                            ('existing_endpoint_uri', str),\n",
    "                            ('deployed_models_count', int),\n",
    "                            ('undeploy_model_needed', str),\n",
    "                            ('deployed_model_list', list),\n",
    "                            ('endpoint_traffic_split', str),\n",
    "]):\n",
    "\n",
    "  from google.cloud import aiplatform\n",
    "  import json\n",
    "  import logging\n",
    "\n",
    "  aiplatform.init(\n",
    "      project=project,\n",
    "      location=location,\n",
    "  )\n",
    "\n",
    "  deployed_model_list = []\n",
    "\n",
    "  logging.info(f\"Searching for model endpoint: {endpoint_name}\")\n",
    "\n",
    "  if aiplatform.Endpoint.list(\n",
    "      filter=f'display_name=\"{endpoint_name}\"'):\n",
    "    '''\n",
    "    Because existing Endpoint found: \n",
    "        (1) will not create new\n",
    "        (2) Need the endpoint uri\n",
    "        (3) Need list of deployed models on this endpoint;\n",
    "        (4) If more than 1 deployed model exists, trigger subsequent conditional step\n",
    "            to undeploy all but 1 (latest) model \n",
    "\n",
    "    '''\n",
    "    logging.info(f\"Model endpoint, {endpoint_name}, already exists\")\n",
    "    create_new_endpoint=\"False\"\n",
    "    \n",
    "    # create endpoint list resource in memory\n",
    "    _endpoint = aiplatform.Endpoint.list(\n",
    "        filter=f'display_name=\"{endpoint_name}\"'\n",
    "    )[0]\n",
    "    logging.info(f\"Parsing details for _endpoint: {_endpoint}\")\n",
    "    \n",
    "    # retrieve endpoint uri\n",
    "    existing_endpoint_uri = _endpoint.resource_name\n",
    "    logging.info(f\"existing_endpoint_uri: {existing_endpoint_uri}\")\n",
    "    _traffic_split = _endpoint.traffic_split\n",
    "\n",
    "    # retrieve deployed model IDs\n",
    "    deployed_models = _endpoint.gca_resource.deployed_models\n",
    "    deployed_models_count = len(deployed_models)\n",
    "    logging.info(f\"deployed_models_count: {deployed_models_count}\")\n",
    "\n",
    "    if deployed_models_count > 1:\n",
    "      # deployed_model_id_0 = _endpoint.gca_resource.deployed_models[0].id\n",
    "      # deployed_model_id_1 = _endpoint.gca_resource.deployed_models[1].id\n",
    "      undeploy_model_needed = \"True\"                                             # arbitrary assumption: no more than 2 (3) models per model_endpoint\n",
    "      for model in deployed_models:\n",
    "        deployed_model_list.append(model.id)\n",
    "    elif deployed_models_count == 0:\n",
    "      undeploy_model_needed = \"False\"\n",
    "    else:\n",
    "      undeploy_model_needed = \"False\"\n",
    "      deployed_model_list.append(_endpoint.gca_resource.deployed_models[0].id)\n",
    "\n",
    "    # deployed_model_id = _endpoint.gca_resource.deployed_models[0].id\n",
    "    logging.info(f\"Currently deployed_model_list {deployed_model_list}\")\n",
    "\n",
    "  else:\n",
    "    logging.info(f\"Model endpoint, {endpoint_name}, does not exist\")\n",
    "    \n",
    "    create_new_endpoint=\"True\"\n",
    "    deployed_models_count=0\n",
    "    existing_endpoint_uri=\"N/A\"\n",
    "    undeploy_model_needed = \"N/A\"\n",
    "    _traffic_split = \"N/A\"\n",
    "    # deployed_model_list = []\n",
    "\n",
    "  logging.info(f\"create_new_endpoint {create_new_endpoint}\")\n",
    "  logging.info(f\"existing_endpoint_uri {existing_endpoint_uri}\")\n",
    "  logging.info(f\"deployed_models_count {deployed_models_count}\")\n",
    "  logging.info(f\"undeploy_model_needed {undeploy_model_needed}\")\n",
    "  logging.info(f\"deployed_model_list {deployed_model_list}\")\n",
    "  logging.info(f\"_traffic_split {_traffic_split}\")\n",
    "\n",
    "\n",
    "  return (\n",
    "      f'{create_new_endpoint}',\n",
    "      f'{existing_endpoint_uri}',\n",
    "      deployed_models_count,\n",
    "      f'{undeploy_model_needed}',\n",
    "      deployed_model_list,\n",
    "      f'{_traffic_split}',\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f8c23c-6099-4e1d-a01c-00ceaa37e55d",
   "metadata": {},
   "source": [
    "### TODO: Create Serving Function for Pretrained Model \n",
    "* see [this example](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/tf_hub_obj_detection/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e507694e-d4de-4abc-b509-a4dbb988b19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08961219-e468-4d89-91a6-107cea3c3853",
   "metadata": {},
   "source": [
    "### TODO: Upload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8acd2cf-3489-457a-a521-514755c4a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile vertex_train/trainer/task.py\n",
    "\n",
    "# @kfp.v2.dsl.component(\n",
    "#     base_image=\"python:3.9\",\n",
    "#     packages_to_install=['google-cloud-aiplatform==1.16.1',\n",
    "#                          'google-cloud-storage',\n",
    "#                          'tensorflow==2.8',\n",
    "#                          'tensorflow-hub==0.12.0',\n",
    "#                          'tensorflow-estimator==2.6.0',\n",
    "#                          'keras==2.6'],\n",
    "#     output_component_file=\"./pipelines/upload_pretrained_model.yaml\",\n",
    "# )\n",
    "# def upload_pretrained_model(\n",
    "#     project: str,\n",
    "#     location: str,  \n",
    "#     saved_model_gcs_bucket: str,\n",
    "#     model_display_name: str,\n",
    "#     serving_container_image_uri: str,\n",
    "# ) -> NamedTuple('Outputs', [('vertex_model', Artifact),\n",
    "#                             ('vertex_model_uri', str),\n",
    "#                             ('vertex_model_display_name', str),\n",
    "#                             ('vertex_model_gcs_dir', str)]):\n",
    "  \n",
    "#   from google.cloud import aiplatform\n",
    "#   import json\n",
    "#   import logging\n",
    "#   import os\n",
    "#   import tensorflow as tf\n",
    "#   import tensorflow_hub as hub\n",
    "#   from datetime import datetime\n",
    "\n",
    "#   aiplatform.init(project=project,location=location)\n",
    "  \n",
    "#   # Load compressed models from tensorflow_hub\n",
    "#   os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
    "\n",
    "#   # TODO: paramaterize\n",
    "#   IMG_HEIGHT = 224\n",
    "#   IMG_WIDTH = 224\n",
    "#   IMG_CHANNELS = 3\n",
    "#   BATCH_SIZE = 32\n",
    "#   NUM_IMAGES = 510\n",
    "#   NUM_NEIGH = 3 # top 3\n",
    "\n",
    "#   # ==============================================================================\n",
    "#   # Load pretrained model from TF Hub\n",
    "#   # ==============================================================================\n",
    "#   layers = [\n",
    "#       hub.KerasLayer(\n",
    "#           \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", # TODO: paramaterize\n",
    "#           input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "#           trainable=False,\n",
    "#           name='mobilenet_embedding'),\n",
    "#       tf.keras.layers.Flatten()\n",
    "#   ]\n",
    "\n",
    "#   model = tf.keras.Sequential(layers, name='img_embedding') # TODO: paramaterize\n",
    "#   print(\"model summary:\", model.summary())\n",
    "  \n",
    "#   TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "#   MODEL_NAME = f'{model_display_name}-{TIMESTAMP}'\n",
    "#   # print(\"MODEL_NAME\", MODEL_NAME)\n",
    "  \n",
    "#   save_path = os.path.join(\"gs://\", saved_model_gcs_bucket, MODEL_NAME)\n",
    "#   print(\"model save_path\", save_path)\n",
    "  \n",
    "#   model.save(save_path)\n",
    "\n",
    "#   # ==============================================================================\n",
    "#   # Load pretrained model from TF Hub\n",
    "#   # ==============================================================================\n",
    "  \n",
    "#   # TODO: parametrize\n",
    "#   SERVING_CONTAINER_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\"\n",
    "\n",
    "#   # Upload model to Vertex\n",
    "#   model = aiplatform.Model.upload(\n",
    "#       display_name=f'{MODEL_NAME}',\n",
    "#       artifact_uri=save_path,\n",
    "#       serving_container_image_uri=SERVING_CONTAINER_IMAGE,\n",
    "#       sync=True,\n",
    "#   )\n",
    "#   print(\"Model uploaded\")\n",
    "#   print(\"model.resource_name:\", model.resource_name)\n",
    "#   print(\"model.display_name:\", model.display_name)\n",
    "\n",
    "#   vertex_model_uri=model.resource_name\n",
    "#   vertex_model_display_name=model.display_name\n",
    "\n",
    "#   return (\n",
    "#       model, \n",
    "#       f'{vertex_model_uri}',\n",
    "#       f'{vertex_model_display_name}',\n",
    "#       f'{save_path}',\n",
    "#   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546429a-d09d-4aaa-8a95-7605719393af",
   "metadata": {},
   "source": [
    "### TODO: Deploy Model\n",
    "Proceeded by pre-built component for creating endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93295830-0e98-4412-b41c-b698c64fd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @kfp.v2.dsl.component(\n",
    "#     base_image=\"python:3.9\",\n",
    "#     packages_to_install=['google-cloud-aiplatform==1.16.1'], # 'tensorflow==2.6'\n",
    "      # output_component_file=\"./pipelines/find_model_endpoint.yaml\",\n",
    "# )\n",
    "# def deploy_pretrained_model(\n",
    "#     project: str,\n",
    "#     location: str,\n",
    "#     model_endpoint_name: str,\n",
    "#     model_resource_path: str,\n",
    "#     model_display_name: str,\n",
    "#     traffic_percentage: int,\n",
    "#     serving_machine_type: str,\n",
    "#     serving_min_replica_count: int,\n",
    "#     serving_max_replica_count: int,\n",
    "# ) -> NamedTuple('Outputs', [('vertex_endpoint', Artifact),\n",
    "#                             ('vertex_model', Artifact),\n",
    "#                             ('vertex_endpoint_uri', str),\n",
    "#                             ('vertex_model_uri', str),]):\n",
    "  \n",
    "#   from google.cloud import aiplatform\n",
    "#   import json\n",
    "#   import logging\n",
    "#   from datetime import datetime\n",
    "\n",
    "#   aiplatform.init(\n",
    "#       project=project,location=location,\n",
    "#   )\n",
    "\n",
    "#   # Find Vertex Endpoint\n",
    "#   endpoint_id = aiplatform.Endpoint.list(filter='display_name={}'.format(model_endpoint_name))[0]\n",
    "#   logging.info(\"endpoint_id: %s\", endpoint_id)\n",
    "\n",
    "#   endpoint_uri = endpoint_id.resource_name\n",
    "#   logging.info(\"endpoint_uri: %s\", endpoint_uri)\n",
    "#   logging.info(\"endpoint_uri[54:]: %s\", endpoint_uri[54:])\n",
    "\n",
    "#   endpoint = aiplatform.Endpoint(endpoint_name=endpoint_uri[54:])\n",
    "#   logging.info(\"endpoint: %s\", endpoint)\n",
    "\n",
    "#   # Initialize Vertex Model\n",
    "#   logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "#   model_resource = aiplatform.Model(model_resource_path)\n",
    "  \n",
    "#   deployed_model = model_resource.deploy(\n",
    "#       endpoint=endpoint,\n",
    "#       deployed_model_display_name=f'deployed-{model_display_name}',\n",
    "#       traffic_percentage=100,                                     # should be 100 since no other models on endpoint yet\n",
    "#       machine_type=serving_machine_type,\n",
    "#       min_replica_count=serving_min_replica_count,\n",
    "#       max_replica_count=serving_max_replica_count,\n",
    "#       sync=True,\n",
    "#   )\n",
    "#   # deployed_model.wait()\n",
    "#   print(\"deployed_model.display_name:\", deployed_model.display_name)\n",
    "#   print(\"deployed_model.resource_name:\", deployed_model.resource_name)\n",
    "\n",
    "#   vertex_endpoint_uri=endpoint.resource_name\n",
    "#   vertex_model_uri=deployed_model.resource_name\n",
    "  \n",
    "#   return (endpoint, \n",
    "#           deployed_model, \n",
    "#           f'{vertex_endpoint_uri}', \n",
    "#           f'{vertex_model_uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022325d2-0105-4bcf-8588-499b426ca18e",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "**TODO**\n",
    "* logic for writting catalog embedding vectors`xx.json` is specific to current image GCS file pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d8e6369-f0ac-4779-bf1b-ee3dfc1cceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=['google-cloud-aiplatform==1.16.1',\n",
    "                         'google-cloud-storage',\n",
    "                         'tensorflow==2.8',\n",
    "                         'tensorflow-hub==0.12.0',\n",
    "                         'tensorflow-estimator==2.8.0',\n",
    "                         'keras==2.8.0'],\n",
    "    output_component_file=\"./pipelines/feature_extraction.yaml\",\n",
    ")\n",
    "def feature_extraction(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    images_gcs_uri: str,\n",
    "    emb_index_gcs_uri: str,\n",
    "    saved_model_gcs_bucket: str,\n",
    "    index_json_name: str,\n",
    "    # model_resource_path: str,\n",
    "    # vertex_model_gcs_dir: str,\n",
    ") -> NamedTuple('Outputs', [('embedding_index_file_uri', str),\n",
    "                            ('saved_pretrained_model_gcs_location', str),\n",
    "                            # ('img_bottleneck_model', Artifact),\n",
    "                            ]):\n",
    "  import os\n",
    "  from google.cloud import storage\n",
    "  from google.cloud.storage.bucket import Bucket\n",
    "  from google.cloud.storage.blob import Blob\n",
    "  from datetime import datetime\n",
    "  import tensorflow as tf\n",
    "  import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "  # Load compressed models from tensorflow_hub\n",
    "  os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
    "\n",
    "  # TODO: paramaterize\n",
    "  IMG_HEIGHT = 224\n",
    "  IMG_WIDTH = 224\n",
    "  IMG_CHANNELS = 3\n",
    "  BATCH_SIZE = 32\n",
    "  NUM_IMAGES = 510\n",
    "  NUM_NEIGH = 3 # top 3\n",
    "\n",
    "  # ==============================================================================\n",
    "  # Define helper functions\n",
    "  # ==============================================================================\n",
    "  \n",
    "  def _upload_blob_gcs(gcs_uri, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to GCS bucket\"\"\"\n",
    "    client = storage.Client(project=project)\n",
    "    blob = Blob.from_string(os.path.join(gcs_uri, destination_blob_name))\n",
    "    blob.bucket._client = client\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "  def read_and_decode(filename, reshape_dims=[IMG_HEIGHT, IMG_WIDTH]):\n",
    "    # Read the file\n",
    "    img = tf.io.read_file(filename)\n",
    "  \n",
    "    # Convert the compressed string to a 3D uint8 tensor.\n",
    "    img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "  \n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    # This makes the img 1 x 224 x 224 x 3 tensor with the data type of float32\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
    "  \n",
    "    # Resize the image to the desired size.\n",
    "    return tf.image.resize(img, reshape_dims)\n",
    "\n",
    "  def create_embeddings_dataset(embedder, img_path):\n",
    "    dataset_filenames = []\n",
    "    dataset_embeddings = []\n",
    "\n",
    "    list_dir = tf.io.gfile.listdir(img_path)\n",
    "\n",
    "    for file in list_dir:\n",
    "      img_tensor = read_and_decode(img_path + \"/\" + file, [IMG_WIDTH, IMG_HEIGHT])\n",
    "      embeddings = embedder(img_tensor)\n",
    "      dataset_filenames.append(img_path + \"/\" + file)\n",
    "      dataset_embeddings.extend(embeddings)\n",
    "  \n",
    "    dataset_embeddings = tf.convert_to_tensor(dataset_embeddings)\n",
    "  \n",
    "    return dataset_filenames, dataset_embeddings\n",
    "\n",
    "  # ==============================================================================\n",
    "  # Download pre-trained model\n",
    "  # ==============================================================================\n",
    "  layers = [\n",
    "      hub.KerasLayer(\n",
    "          \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", # TODO: paramaterize\n",
    "          input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "          trainable=False,\n",
    "          name='mobilenet_embedding'),\n",
    "      tf.keras.layers.Flatten()\n",
    "  ]\n",
    "\n",
    "  model = tf.keras.Sequential(layers, name='pretrained_mobilenet') # TODO: paramaterize\n",
    "  # loaded_model = tf.keras.models.load_model(vertex_model_gcs_dir)\n",
    "  # print(\"model summary:\", loaded_model.summary())\n",
    "  \n",
    "  TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "  MODEL_NAME = f'pipe-mobilenet_v2-{TIMESTAMP}'\n",
    "  print(\"MODEL_NAME\", MODEL_NAME)\n",
    "  \n",
    "  save_path = os.path.join(saved_model_gcs_bucket, MODEL_NAME) # \"gs://\", \n",
    "  print(\"model save_path\", save_path)\n",
    "  \n",
    "  model.save(save_path)\n",
    "\n",
    "  # ==============================================================================\n",
    "  # Create embedding dataset\n",
    "  # ==============================================================================\n",
    "  dataset_filenames, dataset_embeddings = create_embeddings_dataset(\n",
    "    lambda x: model.predict(x),\n",
    "    images_gcs_uri\n",
    "  )\n",
    "\n",
    "  print(\"sample dataset_filenames\", dataset_filenames[:3])\n",
    "  print(\"dataset_embeddings shape:\", dataset_embeddings.shape)\n",
    "\n",
    "  # ==============================================================================\n",
    "  # Write Embeddings and IDs to json\n",
    "  # ==============================================================================\n",
    "\n",
    "  # TODO: this code will only work with the file pattern created from zipped file\n",
    "  #       adjust this to expected GCS file patterns\n",
    "\n",
    "  with open(f\"{index_json_name}\", \"w\") as f:\n",
    "    for gcs_uri, vector in zip(dataset_filenames,dataset_embeddings):\n",
    "      x = gcs_uri.split(\"/\")[-1]\n",
    "      id = x.split(\".\")[0]\n",
    "      vector = vector.numpy()\n",
    "      f.write('{\"id\":\"' + str(id) + '\",')\n",
    "      f.write('\"embedding\":[' + \",\".join(str(x) for x in vector) + \"]}\")\n",
    "      f.write(\"\\n\")\n",
    "\n",
    "  _upload_blob_gcs(emb_index_gcs_uri, f\"{index_json_name}\", f\"{index_json_name}\") \n",
    "\n",
    "  embedding_index_file_uri = f'{emb_index_gcs_uri}/{index_json_name}'\n",
    "  print(\"embedding_index_file_uri:\", embedding_index_file_uri)\n",
    "\n",
    "  return(\n",
    "      f'{embedding_index_file_uri}',\n",
    "      f'{save_path}',\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "320dba26-9206-47ee-b613-db484bfa3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"candidate_embeddings.json\", \"w\") as f:\n",
    "#     for prod, emb in zip(ivm_s,candidate_embeddings):\n",
    "#         f.write('{\"id\":\"' + str(id) + '\",')\n",
    "#         f.write('\"embedding\":[' + \",\".join(str(x) for x in vector) + \"]}\")\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "# with open(f\"{index_json_name}\", \"w\") as f:\n",
    "#   for gcs_uri, vector in zip(dataset_filenames,dataset_embeddings):\n",
    "#     f.write('{\"id\":\"' + str(id) + '\",')\n",
    "#     f.write('\"embedding\":[' + \",\".join(str(x) for x in vector) + \"]}\")\n",
    "#     f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7a123-ac7b-4210-9a59-89373f23d8df",
   "metadata": {},
   "source": [
    "### Create ME Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5185506-7eb3-4946-b524-afa218175431",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=['google-cloud-aiplatform==1.16.1',], # TODO: update once merged\n",
    "    output_component_file=\"./pipelines/create_ann_index.yaml\",\n",
    ")\n",
    "def create_ann_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    staging_bucket: str,\n",
    "    vpc_network_name: str,\n",
    "    emb_index_gcs_uri: str,\n",
    "    dimensions: int,\n",
    "    ann_index_display_name: str,\n",
    "    approximate_neighbors_count: int,\n",
    "    distance_measure_type: str,\n",
    "    leaf_node_embedding_count: int,\n",
    "    leaf_nodes_to_search_percent: int, \n",
    "    ann_index_description: str,\n",
    "    ann_index_labels: Dict, \n",
    ") -> NamedTuple('Outputs', [('ann_index_resource_uri', str),\n",
    "                            ('ann_index', Artifact),]):\n",
    "\n",
    "\n",
    "  from google.cloud import aiplatform\n",
    "  from datetime import datetime\n",
    "\n",
    "  aiplatform.init(project=project, location=location, staging_bucket=staging_bucket)\n",
    "  TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "\n",
    "  ENDPOINT = \"{}-aiplatform.googleapis.com\".format(location)\n",
    "  NETWORK_NAME = vpc_network_name\n",
    "  INDEX_DIR_GCS = emb_index_gcs_uri\n",
    "  PARENT = \"projects/{}/locations/{}\".format(project, location)\n",
    "\n",
    "  print(\"ENDPOINT: {}\".format(ENDPOINT))\n",
    "  print(\"PROJECT_ID: {}\".format(project))\n",
    "  print(\"REGION: {}\".format(location))\n",
    "\n",
    "  ann_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "      display_name=f'{ann_index_display_name}-{TIMESTAMP}',\n",
    "      contents_delta_uri=emb_index_gcs_uri,\n",
    "      dimensions=dimensions,\n",
    "      approximate_neighbors_count=approximate_neighbors_count,\n",
    "      distance_measure_type=distance_measure_type,\n",
    "      leaf_node_embedding_count=leaf_node_embedding_count,\n",
    "      leaf_nodes_to_search_percent=leaf_nodes_to_search_percent,\n",
    "      description=ann_index_description,\n",
    "      labels=ann_index_labels,\n",
    "  )\n",
    "\n",
    "  ann_index_resource_uri = ann_index.resource_name\n",
    "  print(\"ann_index_resource_uri:\", ann_index_resource_uri) \n",
    "\n",
    "  return (\n",
    "      f'{ann_index_resource_uri}',\n",
    "      ann_index,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd83882b-aecd-4e41-b230-9344aacc8fef",
   "metadata": {},
   "source": [
    "### Create Brute Force Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "033b8d54-27e1-4b2e-bf72-03de6f65ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=['google-cloud-aiplatform==1.16.1',], # TODO: update once merged\n",
    "    output_component_file=\"./pipelines/create_brute_force_index.yaml\",\n",
    ")\n",
    "def create_brute_force_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    staging_bucket: str,\n",
    "    vpc_network_name: str,\n",
    "    emb_index_gcs_uri: str,\n",
    "    dimensions: int,\n",
    "    brute_force_index_display_name: str,\n",
    "    approximate_neighbors_count: int,\n",
    "    distance_measure_type: str,\n",
    "    brute_force_index_description: str,\n",
    "    brute_force_index_labels: Dict,\n",
    ") -> NamedTuple('Outputs', [('brute_force_index_resource_uri', str),\n",
    "                            ('brute_force_index', Artifact),]):\n",
    "\n",
    "\n",
    "  from google.cloud import aiplatform\n",
    "  from datetime import datetime\n",
    "\n",
    "  aiplatform.init(project=project, location=location, staging_bucket=staging_bucket)\n",
    "  TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "\n",
    "  ENDPOINT = \"{}-aiplatform.googleapis.com\".format(location)\n",
    "  NETWORK_NAME = vpc_network_name\n",
    "  INDEX_DIR_GCS = emb_index_gcs_uri\n",
    "  PARENT = \"projects/{}/locations/{}\".format(project, location)\n",
    "\n",
    "  print(\"ENDPOINT: {}\".format(ENDPOINT))\n",
    "  print(\"PROJECT_ID: {}\".format(project))\n",
    "  print(\"REGION: {}\".format(location))\n",
    "\n",
    "  brute_force_index = aiplatform.MatchingEngineIndex.create_brute_force_index(\n",
    "      display_name=f'{brute_force_index_display_name}-{TIMESTAMP}',\n",
    "      contents_delta_uri=emb_index_gcs_uri,\n",
    "      dimensions=dimensions,\n",
    "      # approximate_neighbors_count=approximate_neighbors_count,\n",
    "      distance_measure_type=distance_measure_type,\n",
    "      description=brute_force_index_description,\n",
    "      labels=brute_force_index_labels,\n",
    "  )\n",
    "  brute_force_index_resource_uri = brute_force_index.resource_name\n",
    "  print(\"brute_force_index_resource_uri:\",brute_force_index_resource_uri) \n",
    "\n",
    "  return (\n",
    "      f'{brute_force_index_resource_uri}',\n",
    "      brute_force_index,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f8bcc5-6238-4f5c-b3fa-9dd347fd9b40",
   "metadata": {},
   "source": [
    "### Create IndexEndpoint with VPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "669d66a4-356b-4889-8986-b34a4e908a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=['google-cloud-aiplatform==1.16.1',], # TODO: update once merged\n",
    "    output_component_file=\"./pipelines/create_index_endpoint_vpc.yaml\",\n",
    ")\n",
    "def create_index_endpoint_vpc(\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    staging_bucket: str,\n",
    "    vpc_network_name: str,\n",
    "    index_endpoint_display_name: str,\n",
    "    index_endpoint_description: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "                            ('vpc_network_resource_uri', str),\n",
    "                            ('index_endpoint_resource_uri', str),\n",
    "                            ('index_endpoint', Artifact),\n",
    "                            ('index_endpoint_display_name', str),\n",
    "                            ]):\n",
    "\n",
    "  from google.cloud import aiplatform\n",
    "  from datetime import datetime\n",
    "\n",
    "  aiplatform.init(project=project, location=location, staging_bucket=staging_bucket)\n",
    "  TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "  vpc_network_resource_uri = f'projects/{project_number}/global/networks/{vpc_network_name}'\n",
    "  print(\"vpc_network_resource_uri:\", vpc_network_resource_uri)\n",
    "\n",
    "  index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "      display_name=f'{index_endpoint_display_name}-{TIMESTAMP}',\n",
    "      description=index_endpoint_description,\n",
    "      network=vpc_network_resource_uri,\n",
    "  )\n",
    "  index_endpoint_resource_uri = index_endpoint.resource_name\n",
    "  print(\"index_endpoint_resource_uri:\", index_endpoint_resource_uri)\n",
    "\n",
    "  return (\n",
    "      f'{vpc_network_resource_uri}',\n",
    "      f'{index_endpoint_resource_uri}',\n",
    "      index_endpoint,\n",
    "      f'{index_endpoint_display_name}-{TIMESTAMP}'\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4508c3d-1fe4-43f9-9cbe-a47eefe46dcd",
   "metadata": {},
   "source": [
    "### Deploy Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76ff4221-3681-4701-ad62-b6aaef1b25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=['google-cloud-aiplatform==1.16.1',], # TODO: update once merged\n",
    "    output_component_file=\"./pipelines/deploy_ann_index.yaml\",\n",
    ")\n",
    "def deploy_ann_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    staging_bucket: str,\n",
    "    deployed_ann_index_name: str,\n",
    "    ann_index_resource_uri: str,\n",
    "    index_endpoint_resource_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "                            ('index_endpoint_resource_uri', str),\n",
    "                            ('ann_index_resource_uri', str),\n",
    "                            ('deployed_ann_index_name', str),\n",
    "                            ('deployed_ann_index', Artifact),\n",
    "                            ]):\n",
    "  \n",
    "  from google.cloud import aiplatform\n",
    "  from datetime import datetime\n",
    "  TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "  aiplatform.init(project=project, location=location, staging_bucket=staging_bucket)\n",
    "\n",
    "  ann_index = aiplatform.MatchingEngineIndex(\n",
    "      index_name=ann_index_resource_uri\n",
    "  )\n",
    "  ann_index_resource_uri = ann_index.resource_name\n",
    "\n",
    "  index_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
    "      index_endpoint_resource_uri\n",
    "  )\n",
    "\n",
    "  index_endpoint = index_endpoint.deploy_index(\n",
    "      index=ann_index, \n",
    "      deployed_index_id=f'{deployed_ann_index_name}-{TIMESTAMP}'\n",
    "  )\n",
    "\n",
    "  print(index_endpoint.deployed_indexes)\n",
    "\n",
    "  return (\n",
    "      f'{index_endpoint_resource_uri}',\n",
    "      f'{ann_index_resource_uri}',\n",
    "      f'{deployed_ann_index_name}-{TIMESTAMP}',\n",
    "      ann_index,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb200ce7-07b0-42e1-92b1-ee81e92eeda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=['google-cloud-aiplatform==1.16.1',], # TODO: update once merged\n",
    "    output_component_file=\"./pipelines/deploy_brute_index.yaml\",\n",
    ")\n",
    "def deploy_brute_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    staging_bucket: str,\n",
    "    deployed_brute_force_index_name: str,\n",
    "    brute_force_index_resource_uri: str,\n",
    "    index_endpoint_resource_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "                            ('index_endpoint_resource_uri', str),\n",
    "                            ('brute_force_index_resource_uri', str),\n",
    "                            ('deployed_brute_force_index_name', str),\n",
    "                            ('deployed_brute_force_index', Artifact),\n",
    "                            ]):\n",
    "  \n",
    "  from google.cloud import aiplatform\n",
    "  from datetime import datetime\n",
    "  TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "  aiplatform.init(project=project, location=location, staging_bucket=staging_bucket)\n",
    "\n",
    "  brute_index = aiplatform.MatchingEngineIndex(\n",
    "      index_name=brute_force_index_resource_uri\n",
    "  )\n",
    "  brute_force_index_resource_uri = brute_index.resource_name\n",
    "\n",
    "  index_endpoint = aiplatform.MatchingEngineIndexEndpoint(index_endpoint_resource_uri)\n",
    "\n",
    "  index_endpoint = index_endpoint.deploy_index(\n",
    "      index=brute_index, \n",
    "      deployed_index_id=f'{deployed_brute_force_index_name}-{TIMESTAMP}'\n",
    "  )\n",
    "\n",
    "  print(index_endpoint.deployed_indexes)\n",
    "\n",
    "  return (\n",
    "      f'{index_endpoint_resource_uri}',\n",
    "      f'{brute_force_index_resource_uri}',\n",
    "      f'{deployed_brute_force_index_name}-{TIMESTAMP}',\n",
    "      brute_index,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0de6a7-d035-404f-86fc-99343c71aad3",
   "metadata": {},
   "source": [
    "### Query indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35bee283-84ca-460f-b5e2-edb85aeaf339",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=['google-cloud-aiplatform==1.16.1',\n",
    "                         'google-cloud-storage',\n",
    "                         'tensorflow==2.8',\n",
    "                         'tensorflow-hub==0.12.0',\n",
    "                         'tensorflow-estimator==2.8.0',\n",
    "                         'keras==2.8'], # TODO: update once merged\n",
    "    output_component_file=\"./pipelines/query_deployed_indexes.yaml\",\n",
    ")\n",
    "def query_deployed_indexes(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    staging_bucket: str,\n",
    "    num_neighbors: int,\n",
    "    index_endpoint_resource_uri: str,\n",
    "    deployed_brute_force_index_name: str,\n",
    "    deployed_ann_index_name: str,\n",
    "    test_imgs_gcs_dir: str,\n",
    "    num_test_samples: int,\n",
    "    vertex_model_gcs_dir: str,):\n",
    "  \n",
    "  import os\n",
    "  import numpy\n",
    "  import tensorflow as tf\n",
    "  import tensorflow_hub as hub\n",
    "  from google.cloud import aiplatform\n",
    "  from datetime import datetime\n",
    "  \n",
    "  aiplatform.init(project=project, location=location, staging_bucket=staging_bucket)\n",
    "\n",
    "  os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
    "\n",
    "  IMG_HEIGHT = 224\n",
    "  IMG_WIDTH = 224\n",
    "  IMG_CHANNELS = 3\n",
    "  \n",
    "  ##############################################################################\n",
    "  # Helper Functions\n",
    "  ##############################################################################\n",
    "\n",
    "  def read_and_decode(filename, reshape_dims=[IMG_HEIGHT, IMG_WIDTH]):\n",
    "    # Read the file\n",
    "    img = tf.io.read_file(filename)\n",
    "    \n",
    "    # Convert the compressed string to a 3D uint8 tensor.\n",
    "    img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "    \n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    # This makes the img 1 x 224 x 224 x 3 tensor with the data type of float32\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
    "    \n",
    "    # Resize the image to the desired size.\n",
    "    return tf.image.resize(img, reshape_dims)\n",
    "\n",
    "  def create_query_embeddings(embedder, img_path):\n",
    "    dataset_filenames = []\n",
    "    dataset_embeddings = []\n",
    "    list_dir = tf.io.gfile.listdir(img_path)\n",
    "\n",
    "    for file in list_dir[:num_test_samples]:\n",
    "      img_tensor = read_and_decode(img_path + \"/\" + file, [IMG_WIDTH, IMG_HEIGHT])\n",
    "      embeddings = embedder(img_tensor)\n",
    "      dataset_filenames.append(img_path + \"/\" + file)\n",
    "      dataset_embeddings.extend(embeddings)\n",
    "  \n",
    "    dataset_embeddings = tf.convert_to_tensor(dataset_embeddings)\n",
    "    return dataset_filenames, dataset_embeddings\n",
    "\n",
    "  ##############################################################################\n",
    "  # Init IndexEndpoint, Load Model, Create Query embeddings\n",
    "  ##############################################################################\n",
    "\n",
    "  index_endpoint = aiplatform.MatchingEngineIndexEndpoint(index_endpoint_resource_uri)\n",
    "\n",
    "  loaded_model = tf.keras.models.load_model(vertex_model_gcs_dir)\n",
    "  print(\"model summary:\", loaded_model.summary())\n",
    "\n",
    "  query_filenames, query_embeddings = create_query_embeddings(\n",
    "      lambda x: loaded_model.predict(x),\n",
    "      test_imgs_gcs_dir\n",
    "  )\n",
    "  print(\"query_embeddings shape:\", query_embeddings.shape)\n",
    "  print(\"query_filenames:\", query_filenames)\n",
    "  \n",
    "  vector_list = []\n",
    "  for q_vector in query_embeddings:\n",
    "    vector_list.append(q_vector.numpy())\n",
    "\n",
    "  ann_response = index_endpoint.match(\n",
    "      deployed_index_id=deployed_ann_index_name, \n",
    "      queries=vector_list, \n",
    "      num_neighbors=num_neighbors\n",
    "  )\n",
    "  print(\"ann_response:\", ann_response)\n",
    "\n",
    "  brute_force_response = index_endpoint.match(\n",
    "      deployed_index_id=deployed_brute_force_index_name, \n",
    "      queries=vector_list, \n",
    "      num_neighbors=num_neighbors\n",
    "  )\n",
    "\n",
    "  print(\"brute_force_response:\", brute_force_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049b700-ed11-4eeb-892b-8441cb134627",
   "metadata": {},
   "source": [
    "## Build & Compile Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f5701-7b9e-41aa-a6c8-50cf3a165a30",
   "metadata": {},
   "source": [
    "## Pipe Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "601ce354-791f-42e8-8486-4bce6ad458f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_TAG: retail-visual-similarity-v1\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "# if not declared\n",
    "PIPE_USER = 'jtott'\n",
    "BUCKET = 'retail-products-kaggle'\n",
    "BUCKET_URI = f'gs://{BUCKET}'\n",
    "\n",
    "PIPELINE_VERSION = 'v1' # pipeline code\n",
    "PIPELINE_TAG = f'retail-visual-similarity-{PIPELINE_VERSION}'\n",
    "print(\"PIPELINE_TAG:\", PIPELINE_TAG)\n",
    "\n",
    "VERSION = 'v1' # component code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cf40e914-ed16-479a-b52a-abb88e69db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.pipeline(\n",
    "  name=f'{VERSION}-{PIPELINE_TAG}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    staging_bucket: str,\n",
    "    vpc_network_name: str,\n",
    "    images_gcs_uri: str,\n",
    "    emb_index_gcs_uri: str,\n",
    "    saved_model_gcs_bucket: str,\n",
    "    dimensions: int,\n",
    "    ann_index_display_name: str,\n",
    "    approximate_neighbors_count: int,\n",
    "    distance_measure_type: str,\n",
    "    leaf_node_embedding_count: int,\n",
    "    leaf_nodes_to_search_percent: int, \n",
    "    ann_index_description: str,\n",
    "    ann_index_labels: Dict,\n",
    "    brute_force_index_display_name: str,\n",
    "    brute_force_index_description: str,\n",
    "    brute_force_index_labels: Dict,\n",
    "    index_endpoint_display_name: str,\n",
    "    index_endpoint_description: str,\n",
    "    deployed_ann_index_name: str,\n",
    "    deployed_brute_force_index_name: str,\n",
    "    num_neighbors: int,\n",
    "    test_imgs_gcs_dir: str,\n",
    "    num_test_samples: int,\n",
    "    model_endpoint_name: str,\n",
    "    model_display_name: str,\n",
    "    serving_container_image_uri: str,\n",
    "    traffic_percentage: int,\n",
    "    serving_machine_type: str,\n",
    "    serving_min_replica_count: int,\n",
    "    serving_max_replica_count: int,\n",
    "    index_json_name: str,\n",
    "):\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TODO: configure logic for model deployment / rollback / traffic split\n",
    "    # TODO: fix: upload pre-trained model to Vertex Registry\n",
    "    # ========================================================================\n",
    "    \n",
    "#     find_model_endpoint_op = (\n",
    "#         find_model_endpoint_test(\n",
    "#             project=project,\n",
    "#             location=location,\n",
    "#             endpoint_name=model_endpoint_name,\n",
    "#         )\n",
    "#         .set_display_name(\"Find Model Endpoint\")\n",
    "#         .set_caching_options(True)\n",
    "#     )\n",
    "\n",
    "#     create_endpoint_op = (\n",
    "#         gcc_aip.EndpointCreateOp(\n",
    "#             project=project,\n",
    "#             location=location,\n",
    "#             display_name=model_endpoint_name,\n",
    "#         )\n",
    "#         .set_display_name(\"Create Model Endpoint\")\n",
    "#         .set_caching_options(True)\n",
    "#     )\n",
    "\n",
    "#     upload_pretrained_model_op = (\n",
    "#         upload_pretrained_model(\n",
    "#             project=project,\n",
    "#             location=location,\n",
    "#             saved_model_gcs_bucket=saved_model_gcs_bucket,\n",
    "#             model_display_name=model_display_name,\n",
    "#             serving_container_image_uri=serving_container_image_uri,\n",
    "#         )\n",
    "#         .set_display_name(\"Upload Pretrained Model\")\n",
    "#         .after(create_endpoint_op)\n",
    "#         .set_caching_options(True)\n",
    "#     )\n",
    "\n",
    "#     deploy_pretrained_model_op = (\n",
    "#         deploy_pretrained_model(\n",
    "#             project=project,\n",
    "#             location=location,\n",
    "#             model_endpoint_name=model_endpoint_name,\n",
    "#             model_resource_path=upload_pretrained_model_op.outputs['vertex_model_uri'],\n",
    "#             model_display_name=upload_pretrained_model_op.outputs['vertex_model_display_name'],\n",
    "#             traffic_percentage=traffic_percentage,\n",
    "#             serving_machine_type=serving_machine_type,\n",
    "#             serving_min_replica_count=serving_min_replica_count,\n",
    "#             serving_max_replica_count=serving_max_replica_count,\n",
    "#         )\n",
    "#         .set_display_name(\"Deploy Pretrained Model\")\n",
    "#         .after(upload_pretrained_model_op)\n",
    "#         .set_caching_options(True)\n",
    "#     )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Feature Extraction\n",
    "    # ========================================================================\n",
    "    feature_extraction_op = (\n",
    "        feature_extraction(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            images_gcs_uri=images_gcs_uri,\n",
    "            emb_index_gcs_uri=emb_index_gcs_uri,\n",
    "            saved_model_gcs_bucket=saved_model_gcs_bucket,\n",
    "            # model_resource_path=upload_pretrained_model_op.outputs['vertex_model_uri'],\n",
    "            # vertex_model_gcs_dir=upload_pretrained_model_op.outputs['vertex_model_gcs_dir'],\n",
    "            index_json_name=index_json_name,\n",
    "        )\n",
    "        .set_display_name(\"Feature Extraction\")\n",
    "        # .after(deploy_pretrained_model_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    create_ann_index_op = (\n",
    "        create_ann_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            staging_bucket=staging_bucket,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            emb_index_gcs_uri=emb_index_gcs_uri,\n",
    "            dimensions=dimensions,\n",
    "            ann_index_display_name=ann_index_display_name,\n",
    "            approximate_neighbors_count=approximate_neighbors_count,\n",
    "            distance_measure_type=distance_measure_type,\n",
    "            leaf_node_embedding_count=leaf_node_embedding_count,\n",
    "            leaf_nodes_to_search_percent=leaf_nodes_to_search_percent, \n",
    "            ann_index_description=ann_index_description,\n",
    "            ann_index_labels=ann_index_labels,\n",
    "        )\n",
    "        .set_display_name(\"Create ANN Index\")\n",
    "        .after(feature_extraction_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    create_brute_force_index_op = (\n",
    "        create_brute_force_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            staging_bucket=staging_bucket,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            emb_index_gcs_uri=emb_index_gcs_uri,\n",
    "            dimensions=dimensions,\n",
    "            brute_force_index_display_name=brute_force_index_display_name,\n",
    "            approximate_neighbors_count=approximate_neighbors_count,\n",
    "            distance_measure_type=distance_measure_type,\n",
    "            brute_force_index_description=brute_force_index_description,\n",
    "            brute_force_index_labels=brute_force_index_labels,\n",
    "        )\n",
    "        .set_display_name(\"Create Brute Force Index\")\n",
    "        .after(feature_extraction_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Create Index Endpoint\n",
    "    # ========================================================================\n",
    "\n",
    "    create_index_endpoint_vpc_op = (\n",
    "        create_index_endpoint_vpc(\n",
    "            project=project,\n",
    "            project_number=project_number,\n",
    "            location=location,\n",
    "            staging_bucket=staging_bucket,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            index_endpoint_display_name=index_endpoint_display_name,\n",
    "            index_endpoint_description=index_endpoint_description,\n",
    "        )\n",
    "        .set_display_name(\"Create Index Endpoint\")\n",
    "        .after(feature_extraction_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Deploy Indexes\n",
    "    # ========================================================================\n",
    "\n",
    "    deploy_ann_index_op = (\n",
    "        deploy_ann_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            staging_bucket=staging_bucket,\n",
    "            deployed_ann_index_name=deployed_ann_index_name,\n",
    "            ann_index_resource_uri=create_ann_index_op.outputs['ann_index_resource_uri'],\n",
    "            index_endpoint_resource_uri=create_index_endpoint_vpc_op.outputs['index_endpoint_resource_uri'],\n",
    "        )\n",
    "        .set_display_name(\"Deploy ANN Index\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    deploy_brute_index_op = (\n",
    "        deploy_brute_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            staging_bucket=staging_bucket,\n",
    "            deployed_brute_force_index_name=deployed_brute_force_index_name,\n",
    "            brute_force_index_resource_uri=create_brute_force_index_op.outputs['brute_force_index_resource_uri'],\n",
    "            index_endpoint_resource_uri=create_index_endpoint_vpc_op.outputs['index_endpoint_resource_uri'],\n",
    "        )\n",
    "        .set_display_name(\"Deploy Brute Force\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    query_deployed_indexes_op = (\n",
    "        query_deployed_indexes(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            staging_bucket=staging_bucket,\n",
    "            num_neighbors=num_neighbors,\n",
    "            index_endpoint_resource_uri=create_index_endpoint_vpc_op.outputs['index_endpoint_resource_uri'],\n",
    "            deployed_brute_force_index_name=deploy_brute_index_op.outputs['deployed_brute_force_index_name'],\n",
    "            deployed_ann_index_name=deploy_ann_index_op.outputs['deployed_ann_index_name'],\n",
    "            test_imgs_gcs_dir=test_imgs_gcs_dir,\n",
    "            num_test_samples=num_test_samples,\n",
    "            vertex_model_gcs_dir=feature_extraction_op.outputs['saved_pretrained_model_gcs_location'],\n",
    "        )\n",
    "        .set_display_name(\"Query Deployed Indexes\")\n",
    "        .set_caching_options(True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32a29098-2cd7-4a70-ac6c-7d36d1f900ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.v2.compiler.Compiler().compile(\n",
    "  pipeline_func=pipeline, \n",
    "  package_path='custom_container_pipeline_spec.json',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7be15c5f-7935-4a12-b26d-a973397aecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "# MODEL_NAME = f'pipe-mobilenet_v2-{TIMESTAMP}'\n",
    "\n",
    "PREFIX = PIPE_USER    # 'v1'\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "project_number='934903580331'\n",
    "LOCATION = 'us-central1'\n",
    "BUCKET = 'retail-products-kaggle'\n",
    "\n",
    "staging_bucket=f'gs://{BUCKET}/staging'\n",
    "\n",
    "emb_index_gcs_uri = f'gs://{BUCKET}/indexes/{VERSION}'\n",
    "saved_model_gcs_bucket = f'gs://{BUCKET}/saved-models/{VERSION}'\n",
    "vpc_network_name='ucaip-haystack-vpc-network'\n",
    "images_gcs_uri=f'gs://{BUCKET}/{DATA_FOLDER}/train/train'\n",
    "index_json_name = \"retail_kaggle_catalog.json\"\n",
    "\n",
    "# Model\n",
    "model_endpoint_name = f'pipe-mobilenet_v2_endpoint_{VERSION}'\n",
    "model_display_name = f'{PREFIX}-pipe-mobilenet-v2'\n",
    "serving_container_image_uri = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\"\n",
    "traffic_percentage=100\n",
    "serving_machine_type=\"n1-highcpu-32\"\n",
    "serving_min_replica_count=1\n",
    "serving_max_replica_count=1\n",
    "\n",
    "# Indexes\n",
    "DIMENSIONS = 1280\n",
    "approximate_neighbors_count=5\n",
    "distance_measure_type=\"DOT_PRODUCT_DISTANCE\"\n",
    "leaf_node_embedding_count=500\n",
    "leaf_nodes_to_search_percent=7\n",
    "\n",
    "ann_index_display_name = f'ann_{DIMENSIONS}_index_{PREFIX}'\n",
    "brute_force_index_display_name = f'brute_force_{DIMENSIONS}_index_{PREFIX}'\n",
    "\n",
    "ann_index_description=f'Kaggle Retail Product MobileNet_v2 ANN index {VERSION}-{PIPELINE_VERSION}'\n",
    "brute_force_index_description=f\"Kaggle Retail Product MobileNet_v2 (brute force)-{VERSION}-{PIPELINE_VERSION}\"\n",
    "\n",
    "\n",
    "ann_index_labels={'version': f'{VERSION}',\n",
    "                  'pipeline_version': f'{PIPELINE_VERSION}',}\n",
    "\n",
    "brute_force_index_labels={'version': f'{VERSION}',\n",
    "                          'pipeline_version': f'{PIPELINE_VERSION}',}\n",
    "\n",
    "index_endpoint_display_name=f'index_endpoint_{PREFIX}'\n",
    "index_endpoint_description=\"index endpoint description\"\n",
    "\n",
    "deployed_ann_index_name=f'ann_{DIMENSIONS}_deployed_index_{PREFIX}'\n",
    "deployed_brute_force_index_name=f'brute_force_{DIMENSIONS}_deployed_index_{PREFIX}'\n",
    "\n",
    "num_neighbors=3\n",
    "test_imgs_gcs_dir=f'gs://{BUCKET}/{DATA_FOLDER}/test/test'\n",
    "num_test_samples=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c35753b-acfd-48de-8b5d-57cecb6a8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "# overwrite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3ef1bb5-ab76-4353-a655-b64749a32ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/v1-retail-visual-similarity-v1-20220817052335?project=hybrid-vertex\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not PIPELINES.get('train') or overwrite:\n",
    "    response = pipeline_client.create_run_from_job_spec(\n",
    "        job_spec_path='custom_container_pipeline_spec.json',\n",
    "        network=f'projects/{project_number}/global/networks/{vpc_network_name}', # set to same VPC as index\n",
    "        # service_account=SERVICE_ACCOUNT, # <--- TODO: Uncomment if needed\n",
    "        parameter_values={\n",
    "            'project': PROJECT_ID,\n",
    "            'project_number': project_number,\n",
    "            'location': LOCATION,\n",
    "            'staging_bucket': staging_bucket,\n",
    "            'vpc_network_name': vpc_network_name,\n",
    "            'images_gcs_uri': images_gcs_uri,\n",
    "            'emb_index_gcs_uri': emb_index_gcs_uri,\n",
    "            'saved_model_gcs_bucket': saved_model_gcs_bucket,\n",
    "            'dimensions': DIMENSIONS,\n",
    "            'ann_index_display_name': ann_index_display_name,\n",
    "            'approximate_neighbors_count': approximate_neighbors_count,\n",
    "            'distance_measure_type': distance_measure_type,\n",
    "            'leaf_node_embedding_count': leaf_node_embedding_count,\n",
    "            'leaf_nodes_to_search_percent': leaf_nodes_to_search_percent, \n",
    "            'ann_index_description': ann_index_description,\n",
    "            'ann_index_labels': ann_index_labels,\n",
    "            'brute_force_index_display_name': brute_force_index_display_name,\n",
    "            'brute_force_index_description': brute_force_index_description,\n",
    "            'brute_force_index_labels': brute_force_index_labels,\n",
    "            'index_endpoint_display_name': index_endpoint_display_name,\n",
    "            'index_endpoint_description': index_endpoint_description,\n",
    "            'deployed_ann_index_name': deployed_ann_index_name,\n",
    "            'deployed_brute_force_index_name': deployed_brute_force_index_name,\n",
    "            'num_neighbors': num_neighbors,\n",
    "            'test_imgs_gcs_dir': test_imgs_gcs_dir,\n",
    "            'num_test_samples': num_test_samples,\n",
    "            'model_endpoint_name': model_endpoint_name,\n",
    "            'model_display_name': model_display_name,\n",
    "            'serving_container_image_uri': serving_container_image_uri,\n",
    "            'traffic_percentage': traffic_percentage,\n",
    "            'serving_machine_type': serving_machine_type,\n",
    "            'serving_min_replica_count': serving_min_replica_count,\n",
    "            'serving_max_replica_count': serving_max_replica_count,\n",
    "            'index_json_name':index_json_name,\n",
    "        },\n",
    "        pipeline_root=f'{GS_PIPELINE_ROOT_PATH}/{VERSION}',\n",
    "    )\n",
    "    PIPELINES['train'] = response['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358e53d-db80-4a7f-a9a6-efe797634295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ee238-9c32-4daa-813b-291b0d6b55a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b8f43-679b-4a98-b970-b824ba1628b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
