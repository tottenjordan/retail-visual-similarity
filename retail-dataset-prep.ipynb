{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4520327-c600-4dce-95c9-ec4da949cc3a",
   "metadata": {},
   "source": [
    "## Instructions for downloading and preparing the [Kaggle Retail Products Classification](https://www.kaggle.com/competitions/retail-products-classification/data) dataset\n",
    "\n",
    "* Visual Similarity, Matching Engine, Vetex Pipeline [Colab](https://colab.sandbox.google.com/drive/1ysjjGTv7EKkBD90dsdVD3OZvW4Oka1aC#scrollTo=_9U0deUAtD_A)\n",
    "\n",
    "* **Note: if you are creating a Notebook instance and plan to execute vector matching online queries** with Vertex Matching Engine:\n",
    "> * be sure the Notebook instance's network matches the **VPC created for Matching Engine** \n",
    "> * See [here](https://cloud.google.com/vertex-ai/docs/matching-engine/match-eng-setup#vpc-network-peering-setup) if you don't know what this means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f8918-ca74-4885-a295-b3e10f2f8c99",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518e7329-1efa-4c1f-83f8-d5a6628c7d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'  # <--- TODO: CHANGE THIS\n",
    "LOCATION = 'us-central1' \n",
    "!gcloud config set project {PROJECT_ID}\n",
    "\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a67914-9127-49b4-a7d6-6e49451d85d2",
   "metadata": {},
   "source": [
    "If using Google Colab..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9346e-b744-4df2-a395-7842edcb194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "  USER_FLAG = ''\n",
    "else:\n",
    "  USER_FLAG = '--user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8a9363-55fd-4aa8-bff1-eb926d9a8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REMOVE THESE once complete\n",
    "# ! pip3 install $USER kfp google-cloud-pipeline-components --upgrade\n",
    "# !git clone https://github.com/kubeflow/pipelines.git\n",
    "# !pip install pipelines/components/google-cloud/.\n",
    "# !pip install google-cloud-aiplatform==1.10.0\n",
    "# !pip install scann\n",
    "\n",
    "# KEEP: packages needed\n",
    "!pip install google-cloud-aiplatform==1.16.1\n",
    "!pip3 install -U google-cloud-storage $USER_FLAG\n",
    "\n",
    "\n",
    "# Automatically restart kernel after installs\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c1c127-2ec1-4f93-8cfc-eba5c59dea62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiplatform SDK version: 1.16.1\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104b6bb-385a-4b2a-86b7-ec85fa626332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT_ID = 'hybrid-vertex'  # <--- TODO: CHANGE THIS\n",
    "# LOCATION = 'us-central1' \n",
    "# !gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51279baf-0b93-4b99-88d5-dd52d162e711",
   "metadata": {},
   "source": [
    "### Pip & Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f487c90d-1bd9-478b-b7d9-f70eac3e20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# import scann\n",
    "\n",
    "from IPython.display import clear_output, Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "from google.cloud import aiplatform\n",
    "# from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "from google import auth\n",
    "# from google.colab import auth as colab_auth # if using Colab\n",
    "# colab_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384d467-3f05-452b-a4f0-e8a8128f981d",
   "metadata": {},
   "source": [
    "### Setup Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94133367-512a-4ffb-ad58-cd722341adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_CLOUD_PROJECT']=PROJECT_ID\n",
    "\n",
    "# bq_client = bigquery.Client(project=PROJECT_ID, credentials=CREDENTIALS)\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID,location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e84b715-ce49-415b-a256-e4ab71f254e4",
   "metadata": {},
   "source": [
    "## Kaggle Retail Product Classification\n",
    "\n",
    "Download the Kaggle [Retail Product Dataset](https://www.kaggle.com/c/retail-products-classification/data) and upload the zip to a Cloud Storage Bucket\n",
    "* 42,000 product images and short descriptions\n",
    "* 21 product categories\n",
    "* Image dims = 100x100\n",
    "* CSV includes\n",
    "> * `title`: Name of product\n",
    "> * `description`: short description of the product\n",
    "> * `category`: name of the category the product belongs to\n",
    "\n",
    "TODO:\n",
    "* Create text embeddings for description, title, category, and Vision API tags (e.g., OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148dd64a-45ea-427a-af84-36804b55fe79",
   "metadata": {},
   "source": [
    "### Unzip the files and store in Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b30b0c7e-b75b-44de-88e8-0fc008254fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'retail-products-kaggle'\n",
    "ZIP_PATH = 'retail-products-classification.zip' # if the file is gs://$BUCKET/retail-products-classification.zip\n",
    "DEST_FOLDER = 'data-full' # folder to place unzipped files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d2b76e0-de6f-487d-bfe4-dfe30d50002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from zipfile import ZipFile\n",
    "from zipfile import is_zipfile\n",
    "import io\n",
    "\n",
    "def zipextract(bucketname, zipfilename_with_path, destination_folder):\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucketname)\n",
    "\n",
    "    destination_blob_pathname = zipfilename_with_path\n",
    "    \n",
    "    blob = bucket.blob(destination_blob_pathname)\n",
    "    zipbytes = io.BytesIO(blob.download_as_string())\n",
    "\n",
    "    if is_zipfile(zipbytes):\n",
    "        with ZipFile(zipbytes, 'r') as myzip:\n",
    "            for contentfilename in myzip.namelist():\n",
    "                contentfile = myzip.read(contentfilename)\n",
    "                blob = bucket.blob(destination_folder + \"/\" + contentfilename)\n",
    "                blob.upload_from_string(contentfile)\n",
    "\n",
    "# zipextract(\"mybucket\", \"path/file.zip\") # if the file is gs://mybucket/path/file.zip\n",
    "zipextract(BUCKET, ZIP_PATH, DEST_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a07c8-a124-4187-81ae-42eeefafacb1",
   "metadata": {},
   "source": [
    "### Count files in GCS bucket\n",
    "\n",
    "* Train Images = 42,001\n",
    "* Test Images = 6368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "135c40e9-9cee-4037-93fe-5ba71079e5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42001\n"
     ]
    }
   ],
   "source": [
    "# !gsutil du gs://retail-products-kaggle/data-full/train/train | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "313b3fcb-5076-4120-bc59-d45c72eed11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6368\n"
     ]
    }
   ],
   "source": [
    "# !gsutil du gs://retail-products-kaggle/data-full/test/test | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a108043-251d-4165-887f-9ff0f029caee",
   "metadata": {},
   "source": [
    "### Test: compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c22c0192-00f4-4003-a4d9-d765c715fb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://retail-products-kaggle/retail-products-classification.zip\n",
      "gs://retail-products-kaggle/data-full/\n",
      "gs://retail-products-kaggle/dataset/\n"
     ]
    }
   ],
   "source": [
    "IMAGE_DIR = f'gs://{BUCKET}'\n",
    "!gsutil ls $IMAGE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac233a82-d858-49cf-b1f9-11f5a812c54e",
   "metadata": {},
   "source": [
    "#### Validate image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0965fd87-4858-47da-9594-d6d8166256fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gs://retail-products-kaggle/data-full/train/train/097585562X.jpg\n",
    "FILE_NAME = f'{DEST_FOLDER}/train/train/097585562X.jpg' \n",
    "\n",
    "bucket = storage_client.get_bucket(BUCKET)\n",
    "blob = bucket.blob(FILE_NAME)\n",
    "Image(blob.download_as_bytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc63126-02a7-4ded-b4bb-2862cf709a90",
   "metadata": {},
   "source": [
    "## Load compressed model from tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57bb3b9e-5e44-4a60-952c-125b30e326f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e5756-7e13-4ff7-8dee-b73c5c3c1fbe",
   "metadata": {},
   "source": [
    "### Read and decode image - return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c8a3339-d07f-4dcd-a646-bbc87f30824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_decode(filename, reshape_dims=[IMG_HEIGHT, IMG_WIDTH]):\n",
    "  # Read the file\n",
    "  img = tf.io.read_file(filename)\n",
    "  \n",
    "  # Convert the compressed string to a 3D uint8 tensor.\n",
    "  img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "  \n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  # This makes the img 1 x 224 x 224 x 3 tensor with the data type of float32\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
    "  \n",
    "  # Resize the image to the desired size.\n",
    "  return tf.image.resize(img, reshape_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69b9e3a2-c6ab-49f1-9644-c9f99d4e61b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 224, 224, 3), dtype=float32, numpy=\n",
       "array([[[[0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         ...,\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785]],\n",
       "\n",
       "        [[0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         ...,\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785]],\n",
       "\n",
       "        [[0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         ...,\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         ...,\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785]],\n",
       "\n",
       "        [[0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         ...,\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785]],\n",
       "\n",
       "        [[0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         ...,\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785],\n",
       "         [0.9960785, 0.9960785, 0.9960785]]]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_IMG_PATH = f'gs://{BUCKET}/{FILE_NAME}'\n",
    "read_and_decode(TEST_IMG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89551464-1986-4eed-9ad1-2bbecd78c9d6",
   "metadata": {},
   "source": [
    "### Downalod TF Hub model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d724c89c-554f-463c-a334-57d3ebee5a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"visual_embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenet_embedding (KerasL  (None, 1280)             2257984   \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "      hub.KerasLayer(\n",
    "          \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\",\n",
    "          input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "          trainable=False,\n",
    "          name='mobilenet_embedding'),\n",
    "      tf.keras.layers.Flatten()\n",
    "]\n",
    "model = tf.keras.Sequential(layers, name='visual_embedding')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7945cc-8bc2-4bad-8cfe-25bfc6991c9d",
   "metadata": {},
   "source": [
    "# Create embeddings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9a350b7-7830-4854-b203-a7616c65cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def create_embeddings_dataset(embedder, img_path):\n",
    "    dataset_filenames = []\n",
    "    dataset_embeddings = []\n",
    "    list_dir = tf.io.gfile.listdir(img_path)\n",
    "    for file in list_dir[:50]: # test\n",
    "        img_tensor = read_and_decode(img_path + \"/\" + file, [IMG_WIDTH, IMG_HEIGHT])\n",
    "        embeddings = embedder(img_tensor)\n",
    "        dataset_filenames.append(img_path + \"/\" + file)\n",
    "        dataset_embeddings.extend(embeddings)\n",
    "  \n",
    "    dataset_embeddings = tf.convert_to_tensor(dataset_embeddings)\n",
    "  \n",
    "    return dataset_filenames, dataset_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47e29c93-4c0b-4207-bbd0-e8f1fa9d6109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gs://retail-products-kaggle/data-full/train/train/097585562X.jpg', 'gs://retail-products-kaggle/data-full/train/train/097924837X.jpg', 'gs://retail-products-kaggle/data-full/train/train/097965856X.jpg']\n",
      "(50, 1280)\n"
     ]
    }
   ],
   "source": [
    "IMG_PATH = f'gs://{BUCKET}/{DEST_FOLDER}/train/train'\n",
    "\n",
    "dataset_filenames, dataset_embeddings = create_embeddings_dataset(\n",
    "    lambda x: model.predict(x),\n",
    "    IMG_PATH\n",
    ")\n",
    "\n",
    "print(dataset_filenames[:3])\n",
    "print(dataset_embeddings.shape) # should be (NUM_IMAGES, 1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1450ee-2901-4bb8-aa64-34ba113baedc",
   "metadata": {},
   "source": [
    "## Test writting `json` for index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0bc5e61-225c-43c9-988b-3e93117a3d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://retail-products-kaggle/data-full/train/train/097585562X.jpg'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_IMG_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fde0d27-03ac-46d3-8b2c-d88221bc86c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcs_uri: gs://retail-products-kaggle/data-full/train/train/097585562X.jpg\n",
      "x: 097585562X.jpg\n",
      "id_: 097585562X\n"
     ]
    }
   ],
   "source": [
    "gcs_uri =f'{TEST_IMG_PATH}'\n",
    "print(f'gcs_uri: {gcs_uri}')\n",
    "\n",
    "x = gcs_uri.split(\"/\")[-1]\n",
    "print(f'x: {x}')\n",
    "\n",
    "id_ = x.split(\".\")[0]\n",
    "print(f'id_: {id_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9294bf52-3878-4733-97ae-ee005812ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"test.json\", \"w\") as f:\n",
    "    for gcs_uri, vector in zip(dataset_filenames,dataset_embeddings):\n",
    "        x = gcs_uri.split(\"_\")[-1]\n",
    "        id_ = x.split(\".\")[0]\n",
    "        vector = vector.numpy()\n",
    "        f.write('{\"id\":\"' + str(id_) + '\",')\n",
    "        f.write('\"embedding\":[' + \",\".join(str(x) for x in vector) + \"]}\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c4cfc-c74f-4bf8-a979-46f984e6c6cc",
   "metadata": {},
   "source": [
    "# Create Query Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cafe3e0-c37a-4440-90f1-558249534bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TEST_SAMPLES = 50\n",
    "EVAL_IMG_PATH = f'gs://{BUCKET}/{DEST_FOLDER}/test/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6aedb39-aa75-4001-9788-96941fc84bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_embeddings(embedder, img_path, num_test_samples):\n",
    "    dataset_filenames = []\n",
    "    dataset_embeddings = []\n",
    "  \n",
    "    list_dir = tf.io.gfile.listdir(img_path)\n",
    "  \n",
    "    for file in list_dir[:num_test_samples]:\n",
    "        img_tensor = read_and_decode(img_path + \"/\" + file, [IMG_WIDTH, IMG_HEIGHT])\n",
    "        embeddings = embedder(img_tensor)\n",
    "        dataset_filenames.append(img_path + \"/\" + file)\n",
    "        dataset_embeddings.extend(embeddings)\n",
    "  \n",
    "    dataset_embeddings = tf.convert_to_tensor(dataset_embeddings)\n",
    "  \n",
    "    return dataset_filenames, dataset_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "438b300a-3390-41ad-9f38-61c11971ca16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embeddings shape: (50, 1280)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.08451159, 0.        , 0.00108564, ..., 0.3638365 , 0.11259918,\n",
       "       0.        ], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_filenames, query_embeddings = create_query_embeddings(\n",
    "    lambda x: model.predict(x),\n",
    "    EVAL_IMG_PATH,\n",
    "    NUM_TEST_SAMPLES\n",
    ")\n",
    "\n",
    "vector_list = []\n",
    "for q_vector in query_embeddings:\n",
    "    vector_list.append(q_vector.numpy())\n",
    "\n",
    "# print(\"query_filenames:\", query_filenames)\n",
    "print(\"query_embeddings shape:\", query_embeddings.shape) # should be (NUM_TEST_SAMPLES, 1280)\n",
    "# print(\"vector_list shape:\", vector_list.shape)\n",
    "\n",
    "vector_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15284b9b-de04-482b-b1a0-f014afcdc1af",
   "metadata": {},
   "source": [
    "## List Index Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5482bd17-3c01-4705-afac-b4b29fbed2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Listed 0 items.\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai index-endpoints list --project=\"hybrid-vertex\" --region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a5e46c-ebc6-4822-93f5-76a250b81ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_endpoint_resource_uri = 'projects/163017677720/locations/us-central1/indexEndpoints/5129564791202906112'\n",
    "\n",
    "index_endpoint = aiplatform.MatchingEngineIndexEndpoint(index_endpoint_resource_uri)\n",
    "\n",
    "NUM_NEIGH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0e678-2ca2-4ed8-bde8-165fd95ad614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e86ab-a087-4fcb-b8ba-2304acc63754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557baba-c1f7-4946-9c92-e63d7cc02654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1e4f7-c4f2-4510-877a-f93208474231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
